\chapter{Economical Statistics}
\label{chapter:economics}

What is the best way to produce high quality repeated short term predictions at low cost? If you accept my near identification of operational AI with microprediction, or even it's preeminent role, then there cannot be a more important question.   

In Chapter \ref{chapter:oracles} the asymptotic aspirations of an imagined real-world prediction capability led us to a strange answer - with material implications for the way quantitative developers might spend their time. 

In this economic time-out, which is intended to assure us we are on roughly the right track, I argue that there may not be a better framing of the cost and quality issue than the one provided by Friedrich Hayek. 


\section{A thought experiment}

On the back of examples in Chapter \ref{chapter:uses}, let's briefly review {\em the problem} - namely the cost of bespoke artificial intelligence.

We imagine a team of twenty highly trained statisticians, computer scientists and machine learning experts airlifted out of a large corporation and parachuted into a small business, such as a hot dog stand at a football stadium, or an independent bookstore. 

They spend a year understanding the business operation, formulating challenges such as pricing, inventory management, recommendation and customer churn in mathematical terms. 

Working as a close knit team they combine talents and draw on their knowledge of the applied mathematical literature, such as control theory or reinforcement learning, to improve real-time decision making. 

Sooner or later, they drive costs down and revenue up. More hotdogs are sold at a better price. Fewer hotdogs are wasted. Less time is spent responding to complaints lodged at IWasPoisoned.com. Gesture recognition spots sales opportunities in the crowd. Preference for mustard over ketchup is detected using image recognition (tiny stains on the clothes of the approaching customer). And so it goes.     

Now this scenario cannot really play out with good financial results for the obvious reason: cost. So too, similar wholescale optimization cannot be undertaken by the independent bookstore any time soon, and for that matter we can't help the majority of human projects and undertakings - whether for-profit or not. 

But if aliens delivered a box that provided high quality microprediction for a nominal fee, or if there were an equivalently powerful strategy for delivery of the same, it would surely enliven small business, empower inventive people everywhere, speed the creation of novel approaches to solving commercial, environmental and scientific problems, and enrich our lives in ways that are hard to foresee. 

But beginning with our need to meet the school bus, considered in Chapter \ref{chapter:oracles}, we can see that work very often breaks down into two parts. One part comprises simple application logic, whose cost is bounded. Then there is a second more open-ended, ongoing task: search in the world's models and data. In Chapter \ref{chapter:oracles} we began to put some constraints on what an  inexpensive solution to that open-ended task might be. 

The unmet need for inexpensive microprediction will only become more acute. In the next decade the world will spend tens of trillions of dollars manufacturing internet connected devices - and otherwise generating new streams of data facilitating business optimizations driven by microprediction. 

Not only will this create a lot of microprediction tasks, it will also enrich the set of all data that might inform any given one of them. 

The cost of bespoke microprediction is falling but not nearly as fast as the cost of instrumentation. Today you can purchase one dollar's worth of instrumentation - sensors are so cheap they are sometimes tossed from planes. But you cannot purchase one dollar's worth of bespoke quantitative model creation. 

The failure of the economic system to provide a critical economic capital good to the vast numerical majority of businesses and organizations is propagated to the consumer in large and small ways. It impacts the balance of commercial power, the manner in which communities and businesses interact, the diversity of offerings, the safety of goods and services, our real and perceived freedom, our privacy and our wallets.   

If you work in a small business you will appreciate that incremental changes to the cost of custom operational intelligence are not sufficient to rectify this situation. You probably know that the creation of a novel superior technology or product powered by AI is within your imagination, and your capabilities. But will it lurch and groan under the burden of cost? (I've bought that t-shirt). 

Change the cost by a factor of ten and the outlook is very different. Change it by one hundred? One thousand? One million? Is this possible? 

\section{The central problem}

Mico-trade is the hope. Or at least that is how I will phrase it in this chapter. 

In the bus arrival example in Chapter \ref{chapter:oracles} the competing algorithms were playing a repeated game. We can choose to view that as trade, seeking to learn from the economists. The oracle and its prediction suppliers entered an ongoing relationship. The oracle was buying microprediction, and they were selling it. What is interesting about that trade is that it occurs with orders of magnitude less economic friction than conventional trade. 

The lens with which I choose to view this trade was provided by Friedrich Hayek some time ago, and I assert that his generally applicable insights take on a whole new level of significance when applied to the production of microprediction specifically. The role of trade is well appreciated in relation to the central problem of machine learning.

Huh? 

Just checking that you are awake. People normally nod off when economics is mentioned. What is the ``central problem'' of machine learning, you ask, and what could it possibly have to do with trade? Certainly trade might have only a secondary role if the central problem of machine learning was the creation of robots that play pool. But that isn't the central problem of machine learning, is it? 

No, my terminology refers to the central problem of economics - transplanted to data science and microprediction specifically, of course. It is the question of determining the best system for producing and distributing microprediction to all who need it. So when I speak of the central problem of machine learning, I refer to the {\em central economic problem} therein.

Hayek's influential essay {\em The Use of Knowledge in Society}, asks us to confront what the true nature of {\em that} problem might be. Indeed he begins with the question:

\begin{quote}{\cite{Hayek1945TheHayek}}
     What is the problem we seek to solve when we set out to establish a rational economic order? 
\end{quote}

Hayek's perspective emphasizes our hopelessly limited abilities, at least when it comes to gathering all the data necessary to make decisions that optimize complex systems. That knowledge begins in a dispersed state, because as the author notes, ``every individual has some advantage over all others because he possesses unique information of which beneficial use might be made''. 

Obviously that carries over to our challenge. As the prediction web is spun, locality in time, space and other dimensions is important. It is plausible that micro-managers can leverage their neighbor's knowledge, but entirely infeasible to send all the world's data to a central prediction authority for processing - even if they knew what to do with it (and even then, if a tangle of well-meaning regulations allowed for something beyond logistic regression).  

\begin{quote}{\cite{Hayek1945TheHayek}}
The peculiar character of the problem of a rational economic order is determined precisely by the fact that the knowledge of the circumstances of which we must make use never exists in concentrated or integrated form but solely as the dispersed bits of incomplete and frequently contradictory knowledge which all the separate individuals possess.
\end{quote}

Some of that knowledge is not data but technique. Some can't really be sent, because it isn't formalized or quantified. A business owner does not always record every piece of knowledge that informs their next decision. 

They may not even be {\em aware} of all the knowledge that enters that decision, but intuit successfully a sensible course of action (such as changing a price, or decreasing production) that propagates information to others. 

So too, some model-free reinforcement agents of tomorrow (and today) might struggle to relay to a central authority all the reasons for their actions, present past or future. But that does not prevent them from contributing to the global economic brain because their actions - which intuitively increase their expected rewards - signal information to others. 

It isn't clear that any obvious summary of those actions would serve the central authority, who presumably would also need to infer which other agents are learning and responding. Short of the creation of a complete digital twin, which is clearly pointless, what would the central master algorithm do? 

As with the construction of an encyclopedia drawing knowledge from the millions who share it, solving microprediction is better performed by leaving the knowledge in the periphery and allowing agents to perform local calculations, and execute local actions. For the  ``utilization of knowledge which is not given to anyone in its totality'' - in Hayek's words {\em is} the problem.  

(In the case of Wikipedia, some norms and rules are established - but founder Jimmy Wales has expressed skepticism as to how well these work - putting more stock in incentives. So even Wikipedia may be seen to be reward-driven.\endnote{Wikipedia was also directly inspired by Hayek's essay.\cite{wikipedia}}) 


\section{The nature of microprediction}

Keeping the focus on repeated, live short-horizon prediction, is it not striking that we now consider Hayek's knowledge-based argument applied to an economy where the only good is knowledge itself? 

It surely behoves us to account for the very special nature of that good - call it a service if you prefer - although once again, the reader may question my description of the world of AI as a single good economy. (That case continues in Chapter \ref{chapter:decisions} when we consider decision making.)


This sole ``good'' called microprediction, comprises streams of thousands or millions of repeated quantitative predictions of the same type for some bespoke purpose. Not only is microprediction produced repeatedly, in real-time, using whatever knowledge is in the ``vicinity'', but it also {\em represents} whatever knowledge is ``nearby''. 


As noted in Chapter \ref{chapter:oracles} that local knowledge could be human knowledge too, due to the possibility of Turking. 


In speaking about the ``vicinity'', we note that microprediction can encompass the act of providing large numbers of responses to temporal {\em or non-temporal} questions such as “is there a hot dog in this picture?” or implicitly temporal questions like “is row 1332 in this database anomolous (now)?” 

Definitive answers may or may not exist to the questions - as mentioned in Chapter \ref{chapter:uses} - and the truth used to evaluate submissions can depend on the submissions themselves.    


Not everything is included in that representation, as I have been careful to emphasize. The ability to automatically assess modeling is the most useful working definition limiting the scope. For example, ``how many cars will pass 42nd and Park in the next five minutes'' is a type of question that can be asked and answered many times a day and with many variants (such as ``how many cars will pass 34th and 8th between 4:00pm and 4:05pm’’). 


If two traffic prediction algorithms each supply 3,588,480 forecasts - one for each intersection in New York, once every five minutes for a day - then by the end of the day we might already be on our way to determining which is best (though perhaps not which one deals best with holidays ... yet).  

In contrast, comparison of two predictive models each claiming to forecast next year's GDP would call for an entirely different set of considerations, and involve sentient beings trained in statistics and economics. 

There may well be knowledge pertinent to that second, long term question which is not sucked into the micro-manager network nearly as efficiently. There is an indirect path, as micro-managers monitor more longer term competitive means of aggregating information - namely the asset markets. 

(Here there is a distinction that needs to be made between actuarial, frequentist, real-world probability and risk-neutral probability implied by prices. The two converge only in the microprediction limit.)

Small rewards might not work well for singular or long term prediction - unless, possibly, those rewards accumulate across richly cross-sectional data. Asset markets, prediction markets and their ilk seem complementary to the construct we envisage.

So while most dichotomies are false, the distinction between prediction and microprediction is paramount when we consider ``local knowledge'' - the phrase used often to make Hayek's point - just as it is when we consider whether micro-managers will do a good job or not. 

And thus in Chapter \ref{chapter:uses} I steered you from general prediction - which is inherently difficult and expensive - towards applications where ``algorithmic modeling'' works well (to use language from Chapter \ref{chapter:crowd} where the discussion of that divide in statistics is continued). 


That isn't to suggest that microprediction is trivial to solve. But when compared to prediction in general, microprediction should be seen as a tragedy of the commons - and not a fundamental limitation of the universe. 


\section{The nature of micro-decisions}

Repeated decisions are conscious and unconscious. A decision, if repeated, can be construed as conditional microprediction.


One could go further and argue that we are constantly producing micro-probabilities for future scenarios, and that the accuracy or otherwise of conscious or unconscious micro-probability estimation dictates our fates.  

That possibility isn't to be discarded merely because people who make repeated decisions aren't always aware that they are acting in a way that is consistent with some probability, or range thereof. (The foundations of personal probability suggest otherwise, and the definition of a random variable is rather broad.)


Individuals and businesses make decisions of many different kinds, large and small. In the morning we decide which route to take to work. At work we might categorize a customer as loyal or flighty, bid on a work of art, or set the price on a hotel room. At play we might decide whether to throw a slider or a fastball, or steer a video game car left or right.   


On the surface these are very different activities but they can all be reduced to the task of producing micropredictions. By analogy television sets, light bulbs, ceiling fans, clock radios, clothes dryers, coffee makers and computer monitors are diverse uses of a different source of energy: electricity. 


Microprediction certainly isn't electricity yet, and there isn't an equivalent grid ... yet. For now, it is an abstraction that might make it easier to view machine learning, and analytics production and distribution, through an economic lens. 

I dare say there are micro-non-decisions. Microprediction is something we desire even when it is not of {\em any} economic importance, and even when it has no chance of changing our decisions at all. Sometimes I stand zombie-like at the baggage claim, wanting an arrival estimate yet at the same time determined not to alter my actions in any way when the forecast arrives. 

However setting stubbornness aside, micro-decision quality is very directly related to revenue in commercial settings. We saw an example of computing the ratio between the revenue and accuracy changes in Chapter \ref{chapter:uses}. We will draw a connection between accuracy and helpfulness to a trader in Chapter \ref{chapter:scoring}. 

And as elaborated in Chapter \ref{chapter:uses}, demand for microprediction is driven by more than idle curiosity. The capability is a tangible asset helping businesses and organizations create other quality goods and services sold to customers. 

That's true even if the physical form taken by micro-decision capability is up for grabs. All soliciting, rewarding and migration can occur under the covers - not every user needs to care about that. 

 
Or, microprediction might manifest as more explicit communication: a stream of predictions delivered from another party, as in Chapter \ref{chapter:oracles}.

Or, microprediction might involve a hybrid setup, with computation performed predominantly in-house, but residuals leaving the firm in a highly secure manner (see Chapter \ref{chapter:privacy}). 

Or, microprediction capability can be something in between. It can morph and move from one host to another. Value can be stored in model parameters, in a file, in a specification of a neural network or even the vague memory of a human. So long as it can be deployed (see Chapter \ref{chapter:mental}).  

However we come at this, the ongoing ability to predict things repeatedly is absolutely essential to every firm and every individual.  Such is the nature of microprediction. 

Given that, how should the capability, and the micropredictions, be produced and distributed? Does the generalized argument of Hayek, which warns us against economic decisions that are not governed by price, hold water? 

I hope the reader is at least suspecting of the possibility that the best system for arranging production of local knowledge is the one that best harnesses local knowledge. (I doubt that Hayek imagined the opportunity to apply his argument to local knowledge itself!) 


\section{Trying to reduce cost}

The implications are quite profound. I've spent most of my career striving to create fine works of mathematical modeling: business decision tools manifesting as calculators, embedded analytics, algorithmic trading processes, and enterprise data feeds - most of which can be couched as microprediction. 


But I now view that as a centrally planned mode of production simply too expensive for all but the largest companies. Even in large firms, bespoke AI is usually reserved for a small list of important projects. 


Microprediction clearly can't succeed in some asymptotically satisfying sense (the oracle definition from Chapter \ref{chapter:oracles}) if attempted by someone or something who is isolated. In theory, microprediction should involve the entirety of the world's data - which ties us quite tightly to Hayek's concern. 


In theory, microprediction can keep getting better indefinitely, so long as it benefits from new insights and modeling technique. 
 
 
The implication is that microprediction might require the entire world to collectively engineer - something of a desperate measure, it might seem - though consider what has already been tried. 


The problem of cost has resisted many of the usual remedies. Countless hardworking generous people have written excellent freely available open source software that makes the creation of micropredictions easier. This hasn't solved the problem of cost. 

Raising awareness of the possibilities for AI has helped. Recognition of the importance of mathematics has seeped into the public conscious and the highest levels of corporate management in a way that, at the outset of my career, I never imagined possible. 

But the press around the AI revolution has not provided sufficient impetus for a reevaluation of cost. On the contrary, it helps disguise the problem. The problem persists in part because most firms lack a true will to cannibalize their own groups of quant developers in any material way.


Perhaps we are merely at an early stage of the cycle when any use of applied mathematics can be heralded as a success. There is no strong expectation of a Moore's Law for AI as yet, despite the seeming lack of physical constraint.  


Supply is being addressed. A few years back an explosion in online technical education occurred, epitomized by the fact that two million people have enrolled in a single online course titled {\em Machine Learning}, taught by Andrew Ng.\endnote{The mentioned course is offered by Stanford University and Coursera} Massive Open Online Courses help, but might not lead to order of magnitude decreases in the cost of tailored data science.  

Factory line production patterns exist in data science, of a sort.  Data scientists construct pipelines where different transformations to data occur on their way to a final product, just like a Ford Model T.\endnote{See \cite{luigi} for an example of a pipeline tool.} 

There is also an economy for the parts that need to be supplied at each stage of the machine learning pipeline - to extend the analogy - and that sounds like the beginnings of a recipe for a low cost mass produced product. Automatic search for the right pipeline enters the fray. Yet the cost problem persists. 

Is bespoke prediction just inherently expensive?  As noted in Chapter \ref{chapter:introduction} we tend to associate AI activity with medium term, risky, complex projects and highly trained people - making a high cost of microprediction capability seem almost inevitable.


\section{Unbundling microprediction}
 
To reduce cost, we might lean on some commonplace economic insights. While obvious, these haven't really been followed to their logical conclusions in the case of microprediction specifically - because it is hidden. 

There is the benefit of standardizing what is produced and delivered. A startup company inventing a new toaster doesn't have to think about how to generate electricity, and has a very good idea of what voltage to expect. A developer should be able to create applications that are very light on the quant libraries and iterative work, yet draw enough power elsewhere. 


Standardization reduces cost in another way. Because the engineer designing a wind powered turbine doesn't need to know that it is powering your toaster, it is easier for her to bring down your cost of toast as well. So too, creators of autonomous algorithms that roam a microprediction network shouldn't have to think about every possible application. 

But when we consider the potential for standards as they apply to intelligent applications, and the inability of algorithms to latch onto them, the biggest hurdle is the fact that commerce is driven by microprediction {\em implicitly}, not explicitly. 


For now, microprediction is bundled and disguised in recommendation, pricing, inventory management, industrial control, scheduling, image recognition, navigation, anomaly detection and human oversight - to name a few examples from Chapter \ref{chapter:uses}.

We tend not to bucket natural language processing, intrusion detection, predictive maintenance and combinatorial chemistry together. All these seemingly disparate activities constitute the artificial intelligence revolution we read about - though the various terms of art belie a microprediction abstraction that helps illuminate the economic failure. Is {\em bundled microprediction} produced efficiently? 

Unbundled microprediction can be a better path. Companies might become increasingly reliant on microprediction in a more direct way, over time. This can facilitate a critical mass of people and machines adopting similar standards. They can adopt microprediction as a first class abstraction - a self imposed bottleneck between the leveraging of microprediction in applications and the sourcing of the predictive power. 

In this scenario companies adopt minimalist protocols for repeated quantitative tasks. They encourage a lattice of microprediction tasks stretching within and between firms. When disagreements over data models exists, the algorithms are smart enough to jump from one railway gauge to another

In this scenario bespoke microprediction {\em can} be produced cheaply and at scale. Microprediction {\em ought} to be as useful as electricity, to as many people. Firms can, through trade in microprediction, unlock the enormous latent economic value locked in the now, as it were: the potential for sharing and reuse of data and features.


\section{Gambling on abstraction?}

It may come to pass that a few companies re-express diverse problems into a common microprediction abstraction, and use protocols and software connecting supply and demand of the same. They may adopt a small number of standardized games, whose rules and incentives are broadly understood, thus encouraging ruthless algorithmic competition.

Abstraction does not come for free, I hasten to add.  Consider some examples that motivate the prediction network, such as the internet protocols themselves, or if you prefer, the World Wide Web. There was, and is, a cost to these abstractions. There are some things you can do with the web but many things you cannot. Had different agreements been reached, the entire web might have looked like Wikipedia (for better, or worse). 


There are opportunity costs of standards lower down too, such as many possibilities for vertical optimization all the way from delivery of packets to smart apps. There has always been the likelihood of ossification in protocols or supporting technology. 


But it seems the cost of abstraction was worth it. The Web, in particular, unleashed the creativity of a billion people. And germane to our discussion of search, the Web crowd-sourced a vast collection of relationships between documents - the backlinks created by web page authors. 

The importance of this structure - the links between pages - was not immediately recognized. Later, it became a prerequisite for high quality, low cost document search. Though document search has come to be dominated by one company, it has nonetheless been revealed as a ``universally'' collective activity. (Back then, {\em everyone} with two fingers and some HTML participated in link creation. Now we also create structure in many other ways.)  


The economic significance of that collective feat is well appreciated, I think, and so too the value of context. Indeed the economic price of our virtual electronic location as we traverse this web is determined by micro-managers of a sort already, as they respond to offers to place advertisements. 


And needless to say, the immense value of low cost high quality document search is only partially reflected in the enterprise value of Google - a number grossly underestimating the total productivity boost to all organizations and individuals.   

I don't want to stretch the analogy - there are important differences. But microprediction is search too - search in the space of models and data. Search for causal relationships, which for the most part can be categorized as local knowledge. Hayek suggests that we stand as much chance of solving this search in a centralized fashion as a librarian charged with organizing all the world's documents. 

It is more likely that causal relationships, both explicit and implicit (in the actions of micro-managers seeking to predict things for reward) will emerge from battles in the periphery. 

Some acknowledgement of the collective nature of the task is essential. I hope that companies gamble on abstraction, and internal practices, that makes it easier to arrange these fights.


\section{Opportunism is good}

If that gamble is made, then I've suggested that micro-managers can match problems to algorithms, and algorithms to data, without human intervention. They can be the ball bearings that enable smooth operation and falling costs.

Value won't be injected into the network only by Ph.D.'s developing cutting edge techniques which are published in the top journals. On the contrary, tiny automated middle-people can be authored by anyone.

We should respect opportunism in both the authors and their product. Speaking to the knowledge possessed by the real estate broker whose ``knowledge is almost exclusively one of temporary opportunities'', Hayek writes:

\begin{oldquote}
    It is a curious fact that this sort of knowledge should today be generally regarded with a kind of contempt and that anyone who by such knowledge gains an advantage over somebody better equipped with theoretical or technical knowledge is thought to have acted almost disreputably. 
\end{oldquote}
This applies rather obviously to exogenous data. But to focus on model search momentarily, it is apparent that we have tens of thousands of open source repositories to choose from, often containing the latest discoveries. They are mostly, woefully underutilized. 

What we are lacking is low cost ``disreputable'' middle-men and women who can lubricate the system by spotting arbitrage opportunity of a new kind - the utility of an obscure piece of code, or combination of techniques, or the application of new technique to an area where it was previously not appreciated. Hayek continues:

\begin{oldquote}
To gain an advantage from better knowledge of facilities of communication or transport is sometimes regarded as almost dishonest, although it is quite as important that society make use of the best opportunities in this respect as in using the latest scientific discoveries.
\end{oldquote}

Some would say the academic system serves this purpose. There is some reward offered, in the sense that it is easier to move ideas from one field to another sometimes, than to break entirely new ground (and you still get published).  

But I would say this transfer is quite slow - certainly slower than the middle-people we have in mind that are inanimate, and ceaselessly working for watts, not salaries. 

In the next chapter I shall introduce some middle-folks (building on the bus arrival oracle example) whose intelligence is admittedly limited. Yet we need middle-machines like this. Hayek warns us against an inclination towards academic snobbery when we assess their merits to society at large.  

\begin{oldquote}
      ... economists who regard themselves as definitely immune to the crude materialist fallacies of the past constantly commit the same mistake where activities directed toward the acquisition of such practical knowledge are concerned—apparently because in their scheme of things all such knowledge is supposed to be “given.” 
\end{oldquote}

That practical knowledge can arise when an algorithm that is not particularly clever crawls from one place to another and, as Thomas Edison put it, fails its way to success. I describe one such setup in Chapter \ref{chapter:mental}.

Returning to data search, Hayek hones in on the real problem for us. When writing statistical papers we have the luxury of comparing one model against another, with the presumption that they will both see the same data. However if one wishes to compare any kind of centralized approach to a collection of micro-managers effecting prediction, this is no longer the case. 


For the combined selfish actions of the micro-managers will materially change what data is seen. As I noted in Chapter \ref{chapter:oracles}, the oracle aspires not to the statistical property of asymptotic efficiency, but to a practical objective where the data set is not fixed. (Nor the other micro-managers whose talents it might draw upon). 

\begin{oldquote}
The common idea now seems to be that all such knowledge should as a matter of course be readily at the command of everybody, and the reproach of irrationality leveled against the existing economic order is frequently based on the fact that it is not so available. This view disregards the fact that the method by which such knowledge can be made as widely available as possible is precisely the problem to which we have to find an answer.
\end{oldquote}

So no, the problem isn't the invention of an algorithm for processing data that is assumed to be present in its entirety (an assumption we find even in many papers addressing decentralized learning). The problem is orchestrating a rational order in which there is even a small chance of benefiting from exogenous data or technique we'd otherwise never encounter. 

Now I don't expect our micro-manager will literally ``see'' that extra data  in all cases but, should we be lucky, the {\em predictive value} of that far-flung knowledge will benefit us. However the means by which it does so may be rather complex because calculations will be performed by many parties we never know. If you squint, Hayek might even be suggesting that we study how to perform prediction using data held privately by multiple parties, without anyone revealing anything to anyone else - see Chapter \ref{chapter:privacy}.

\section{Statistical agency}

If we accept that models, data and causality are far-flung, and if we accept our limited ability to gather it all together, then we are almost compelled to grant the models agency - just as we might citizens of a market economy. 

We are compelled to set them free, so they can discover their own destiny, make their own connections, accumulate local knowledge, and undoubtedly, tie their economic decisions to price. 

When that primary coordinating mechanism is price, abstractly, it may help to consider a special case or two where it is explicitly a price of statistical precision (as with the precision trader in Chapter \ref{chapter:mental}). 


This is not a requirement. The main thing is that we ought not overlook the power of the mechanism.  That is, according to Hayek, an easy mistake to make. 

\begin{oldquote}
I am convinced that if it were the result of deliberate human design, and if the people guided by the price changes understood that their decisions have significance far beyond their immediate aim, this mechanism would have been acclaimed as one of the greatest triumphs of the human mind. 
\end{oldquote}

The marvel of the price mechanism lies in the manner that a global optimization is performed using only local optimization, and local knowledge, and in the fact that only a single quantity (price) constitutes a ``sufficient statistic'' for decision. This informational efficiency is one reason that quantitative business optimization can benefit - just as a consumer good is manufactured from many components, sourced from many suppliers. 

Sharing is facilitated. AI breaks down into microprediction, and that naturally fans out into sub-tasks. Predicting hotel bookings demands constituent micropredictions for weather, airlines, conventions and so forth - in turn informed by other micropredictions. There is no requirement that all of these constituent calculations be performed by employees of the same company. 


In theory, the micropredictions in this supply chain are resuable. Micropredictions created for hotels are likely relevant to microprediction of sales in a nearby bookstore or HVAC optimization in a hospital building across the street. The user should not be required to anticipate that something entirely unrelated can help - such as data created by a sewage overflow management system.  

In theory, self-optimizing supply chains can affect this sharing and should work especially well given that the good in question can be replicated without cost. Parties requiring a common prediction buy it from a producer. Competition forces the producer to pass on the savings from sharing to the consumers of microprediction.  

Motivated by these considerations, I have tried to frame the central task rather narrowly: turn theory into practice by eliminating friction. Provide the scaffolding (code, conventions) for a microprediction economy powered by microscopic statistical firms. 

It's just that we don't need to confuse abstract trade with conventional trade. Sometimes all that is needed is a thin wrapper around existing analytics - one that allows the algorithm to drive from one game to the next. Algorithms that crawl are easy enough to make and modify, as we'll see in Chapter \ref{chapter:mental}. 


\section{Fleeting knowledge}

Despite the ability to effect feature sharing, selection, transformation, pipeline formation and many other statistical miracles (the mediation of Chapter \ref{chapter:mental}), the price mechanism doesn't make many top ten algorithm lists. It doesn't sit there beside the Fourier Transform, the Fast Multipole Algorithm or the Simplex Method.

In fairness, we can't expect the Society of Industrial and Applied Mathematics to include the price mechanism in the top ten algorithms of the 20th Century - trade is 150,000 years old. And age aside, Hayek suggests that our reluctance to recognize its beauty may also be a function of our living within something that we accidentally created.    
\begin{oldquote}
Its misfortune is the double one that it is not the product of human design and that the people guided by it usually do not know why they are made to do what they do. 
\end{oldquote}
In respect to our topic, the price mechanism is surely subject to a triple misfortune, because the use of price mechanism to determine probabilities can be considered distasteful (in counties founded by Puritans, for example). 

If we are inclined to discount this wondrous mechanism, then my task of convincing you might be harder. But I am not dissuaded and there is one more reason to consider Hayek's considerations potent. Knowledge is fleeting. 

Perhaps more than any other consideration, this is why waiting for a large command and control army of data scientists to provide you accurate bespoke microprediction, at low cost, might not be the best strategy. There simply isn't time. 

When it comes to organizing every {\em fleeting} piece of information that impacts a real-time decision, nothing short of a hyper-efficient micro-economy seems up to the task. In contrast, the inadequacy of centralized management of AI will only become more apparent as the complexity of its constituent good we consider, microprediction, increases. 

The Internet-of-Things is filling with this real-time data - more than can possibly be organized by any single person or team. More than can be stored. It is therefore an orthodox economic position that microprediction, and thus AI in large part, will be orchestrated not by hierarchies of human managers but by the combined selfish actions of competing algorithms. 

\begin{quote}{\cite{Hayek1945TheHayek}}
This is not a dispute about whether planning is to be done or not. It is a dispute as to whether planning is to be done centrally, by one authority for the whole economic system, or is to be divided among many individuals... 

Which of these systems is likely to be more efficient depends mainly on the question under which of them we can expect that fuller use will be made of the existing knowledge
\end{quote}
Now I agree we are not close to a point where it is clear which system is superior. On planet machine learning, the Berlin Wall is still standing. 

Low expectations don't help. We are in the nascent stages of the statistical revolution, and anything goes. Nobody allows the errors of their models to be subject to competitive analysis, as I suggest in Chapter \ref{chapter:privacy}. But over time, even small to medium sized enterprises will demand a superior product - which is to say more accurate sequences of predictions at lower and lower cost. 

At that time, the system that best makes use of ``local'' data and modeling expertise may be revealed as fundamentally superior.  

\section{Summary}

Mathematicians like to give economists a hard time. Some think their only contribution is inventing different words for ``derivative'' or ``Lagrange multiplier''. But I have noted Hayek's insight, and transferred it to our stylized economy where there is only one service, namely microprediction. 

In the application of artificial intelligence to business, it isn't so interesting to ask whether data hungry techniques will work given sufficient data - that is true almost by definition. This trivialization is unfair in many respects, and yet analogous to Hayek's discounting of the ``central calculation'' task: 
\begin{quote}{\cite{Hayek1945TheHayek}}
If we possess all the relevant information,
if we can start out from a given system of preferences, and
if we command complete knowledge of available means, the problem which remains is purely one of logic. 
\end{quote}
The hard part of the ``easy half of statistics'' is drawing together the scattered, often short-lived ``local knowledge'' dispersed amongst the world's minds and machines. In particular the ``special knowledge of circumstances of the fleeting moment not known to others'' must somehow enter the picture. 

Hayek advances the price mechanism as the solution to what is now termed the local knowledge problem. But if Hayek was impressed by the price mechanism's decentralized optimization, imagine how he would enjoy the possibility of trade in a good that is itself as close to local knowledge as one can imagine. 

Then, consider how much more forceful his arguments might have been in the presence of near-frictionless trade - where the entire life cycle of a relationship is driven by autonomous, interacting algorithms.  

This leaves open the question of exactly how this might be engineered, of course, but it should at least move us into the right category of solution. I propose that the task of providing ``economical'' statistics (meaning cheap) is solved by encouraging ``economical statistics'' (meaning statistical tools endowed with economic agency).  

So, in the next chapter we turn to the art of wrapping little pieces of statistics with enough economic sense that they can recombine, and self-organize into supply chains for microprediction.  





 











